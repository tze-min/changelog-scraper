{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6306ea1a",
   "metadata": {},
   "source": [
    "# loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31346041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import undetected_chromedriver.v2 as uc\n",
    "\n",
    "apps = pd.read_csv(\"apps_of_interest.csv\")\n",
    "links = apps[\"appstore_link\"]\n",
    "\n",
    "def extract_twitter_id(link):\n",
    "    id = link.split(\"/\")[6]\n",
    "    return id.split(\"id\")[1].split(\"?\")[0]\n",
    "\n",
    "id = links.apply(extract_twitter_id)\n",
    "\n",
    "apps[\"id\"] = id\n",
    "\n",
    "saved_csv = pd.read_csv(\"sample_data.csv\")\n",
    "saved_ids = saved_csv[\"apple_id\"]\n",
    "unique_all_ids = apps[\"id\"].unique().astype(int)\n",
    "unique_saved_ids = saved_ids.unique().astype(int)\n",
    "ids = np.setdiff1d(unique_all_ids, unique_saved_ids, assume_unique = True).tolist()\n",
    "\n",
    "def save_cookie(driver, path):\n",
    "    with open(path, 'w') as filehandler:\n",
    "        json.dump(driver.get_cookies(), filehandler)\n",
    "\n",
    "def load_cookie(driver, path):\n",
    "    with open(path, 'r') as cookiesfile:\n",
    "        cookies = json.load(cookiesfile)\n",
    "    for cookie in cookies:\n",
    "        if 'sameSite' in cookie:\n",
    "            if cookie['sameSite'] == 'None':\n",
    "                cookie['sameSite'] = 'Strict'\n",
    "        driver.add_cookie(cookie)\n",
    "\n",
    "def get_date(dt):\n",
    "    try: \n",
    "        return dt.split(\"(\")[1][:-1]\n",
    "    except IndexError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcec870",
   "metadata": {},
   "source": [
    "# scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ CONFIGURATION ###############\n",
    "\n",
    "# these are start and end indices for apps\n",
    "# this code runs 3 apps each batch\n",
    "# if the code crashes, you can restart for that particular batch (the last batch the code had been trying\n",
    "# to run will be indicated in the printed output below this code cell) by replacing the \"start\" value \n",
    "# with the index you want to begin to run from\n",
    "\n",
    "# you can try running this code cell with start, end = 6000, 6003 to see an example. the output will show up in the\n",
    "# folder \"to-run\" as saved6000to6003.csv; please check there are three app IDs in the csv output. Chrome will open\n",
    "# up a new tab with each batch of apps you're running. \n",
    "\n",
    "start, end = 6000, 8000\n",
    "\n",
    "# App Annie log in details to key in\n",
    "\n",
    "APPANNIE_EMAIL = \"\"\n",
    "APPANNIE_PASSWORD = \"\"\n",
    "\n",
    "# If you receive Error 500 on the webpage that shows up, increase the wait time (in seconds) between scraping each app\n",
    "# by more seconds; can play around with it until App Annie no longer blocks you\n",
    "# but might want to reduce once you're off the hook or the scraping will take longer\n",
    "\n",
    "minwait = 60     # e.g. 70\n",
    "maxwait = 90     # e.g. 100\n",
    "\n",
    "############ SCRAPER ###############\n",
    "\n",
    "BATCH_SIZE = random.randint(3,4)\n",
    "\n",
    "unique_saved_ids = saved_ids.unique().astype(int)\n",
    "unique_all_ids = apps[\"id\"].unique().astype(int)\n",
    "ids_full = np.setdiff1d(unique_all_ids, unique_saved_ids, assume_unique = True).tolist()\n",
    "\n",
    "numAll = len(unique_all_ids)\n",
    "numToScrape = len(ids) - 1\n",
    "print(\"number of apps to scrape in total:\", numAll)\n",
    "print(\"number of apps left to scrape:\", numToScrape)\n",
    "\n",
    "for batch in range(start, end, BATCH_SIZE):\n",
    "    print(\"starting batch\", str(batch))  \n",
    "    \n",
    "    # prepare for scraping\n",
    "    base_url = \"https://www.appannie.com/apps/ios/app/\"\n",
    "    add_url = \"/details?date=!(%272021-10-10%27,%272022-01-01%27)&granularity=weekly&country_code=WW\"\n",
    "    options = uc.ChromeOptions()\n",
    "    driver = uc.Chrome(options = options)\n",
    "    cookies_path = \"cookies.json\"\n",
    "    all_ids, all_versions, all_dates, all_descriptions, missing_apps = [],[],[],[],[]\n",
    "    \n",
    "    if os.path.exists(cookies_path):\n",
    "        os.remove(cookies_path)\n",
    "    \n",
    "    # loop through each app via its ID\n",
    "    ids = ids_full[batch:batch + BATCH_SIZE]\n",
    "    removed, not_removed = [], []\n",
    "    \n",
    "    for idx in tqdm(range(len(ids))):\n",
    "        \n",
    "        # access App Annie page\n",
    "        i = ids[idx]\n",
    "        url = \"\".join([base_url, str(i), add_url])\n",
    "        driver.get(url)\n",
    "        \n",
    "        # if cookies containing login info doesn't exist, log in and save cookie\n",
    "        if not os.path.exists(cookies_path):\n",
    "            sleep(random.uniform(0,1))\n",
    "            username = driver.find_element_by_xpath(\"/html/body/div[2]/div/div/div[1]/div/div[3]/form/div/div[1]/input\")\n",
    "            password = driver.find_element_by_xpath(\"/html/body/div[2]/div/div/div[1]/div/div[3]/form/div/div[2]/input\")\n",
    "            username.send_keys(APPANNIE_EMAIL)\n",
    "            sleep(random.uniform(0,1))\n",
    "            password.send_keys(APPANNIE_PASSWORD)\n",
    "            sleep(random.uniform(3,5))\n",
    "            driver.find_element_by_xpath(\"/html/body/div[2]/div/div/div[1]/div/div[3]/form/div/button\").click() # log in button\n",
    "            save_cookie(driver, cookies_path)\n",
    "        else:\n",
    "            sleep(random.uniform(50,70))\n",
    "            load_cookie(driver, cookies_path)\n",
    "            \n",
    "        #get content in What's New, first check if it's still present in the app store\n",
    "        try:\n",
    "            xp = \"/html/body/div[2]/div/div[2]/div[1]/div[3]/div[2]/div[2]/div[3]/div[2]/div[1]/div[3]/dl\"\n",
    "            table = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, xp)))\n",
    "            not_removed.append(i)\n",
    "        except TimeoutException as e:\n",
    "            # if it's not present in the app store, use a different xpath\n",
    "            try:\n",
    "                xp = \"/html/body/div[2]/div/div[2]/div[1]/div[3]/div[2]/div[2]/div[3]/div[3]/div[1]/div[3]/dl\"\n",
    "                table = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, xp)))\n",
    "                removed.append(i)\n",
    "            # but if app doesn't exist in App Annie, webdriver automatically closes\n",
    "            except TimeoutException as ex:\n",
    "                sleep(random.uniform(50,70))\n",
    "                driver.close()\n",
    "                options = uc.ChromeOptions()\n",
    "                driver = uc.Chrome(options = options)\n",
    "                if os.path.exists(cookies_path):\n",
    "                    os.remove(cookies_path)\n",
    "                continue\n",
    "                             \n",
    "        # save content\n",
    "        vers = table.find_elements_by_css_selector(\"dt\")\n",
    "        vers_text = [x.text for x in vers]\n",
    "        versions_only = [x.split(\" \")[1] for x in vers_text]\n",
    "        dates_only = [get_date(x) for x in vers_text]\n",
    "\n",
    "        desc = table.find_elements_by_css_selector(\"dd\")\n",
    "        desc_text = [x.text for x in desc]\n",
    "\n",
    "        # add content to lists of all_\n",
    "        all_ids += [i] * len(versions_only)\n",
    "        all_versions += versions_only\n",
    "        all_dates += dates_only\n",
    "        all_descriptions += desc_text\n",
    "        \n",
    "        # crawl-delay between each app\n",
    "        if idx < len(ids) - 1:\n",
    "            sleep(random.uniform(minwait, maxwait))\n",
    "        else:\n",
    "            sleep(random.uniform(15, 20))\n",
    "        \n",
    "    driver.close()\n",
    "    if os.path.exists(cookies_path):\n",
    "        os.remove(cookies_path)\n",
    "        \n",
    "    # save batch job\n",
    "    df = pd.DataFrame({\"apple_id\": all_ids, \"version\": all_versions, \"date\": all_dates, \"description\": all_descriptions})\n",
    "    df.to_csv(\"\".join([\"saved_\", str(batch), \"to\", str(batch + BATCH_SIZE), \".csv\"]), index=False)\n",
    "    print(\"rows:\", len(df))\n",
    "    print(\"apps removed from app store:\", removed)\n",
    "    print(\"unable to get data for apps:\", missing_apps)\n",
    "\n",
    "    # crawl-delay between each batch\n",
    "    sleep(random.uniform(70, 120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
